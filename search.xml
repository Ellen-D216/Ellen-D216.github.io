<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2021/08/21/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>回归大家族</title>
    <url>/2021/08/21/%E5%9B%9E%E5%BD%92%E5%A4%A7%E5%AE%B6%E6%97%8F/</url>
    <content><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>公式： $\hat{y}=\theta_0+\theta_1x_1+\theta_2x_2+…+\theta_nx_n$</p>
<p>向量形式：$\hat{y}=h_\theta(\vec{x})=\vec{\theta_0^T}\cdot \vec{x}$</p>
<p>损失函数：$MSE(\vec{X},h_\theta)=\frac{1}{m}\sum_{i=1}^m(\vec{\theta_0^T}\cdot \vec{x^{(i)}}-y^{(i)})$</p>
<p>通过最小化MSE球的参数$\theta$，可得标准方程：$\hat{\theta}=(X^TX)^{(-1)}X^Ty$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = <span class="number">2</span> * np.random.rand(<span class="number">100</span>,<span class="number">1</span>)</span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(<span class="number">100</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X, y)</span><br></pre></td></tr></table></figure>
<pre><code>LinearRegression()
</code></pre><h2 id="sklearn中线性回归的重要属性和方法"><a href="#sklearn中线性回归的重要属性和方法" class="headerlink" title="sklearn中线性回归的重要属性和方法"></a>sklearn中线性回归的重要属性和方法</h2><ul>
<li><code>intercept_</code>表示偏置项</li>
<li><code>coef_</code>表示权重</li>
<li><code>fit(X, y)</code>表示拟合过程</li>
<li><code>predict(X, y)</code>表示预测过程</li>
<li><code>score(X, y)</code>对于线性回归表示相关系数</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lin_reg.intercept_,lin_reg.coef_</span><br></pre></td></tr></table></figure>
<pre><code>(array([3.93410448]), array([[2.94751289]]))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, c=<span class="string">&#x27;b&#x27;</span>, s=<span class="number">2</span>)</span><br><span class="line">plt.plot(X, lin_reg.predict(X), color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.text(<span class="number">1.6</span>,<span class="number">4</span>,<span class="string">&#x27;score:&#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(lin_reg.score(X,y)))</span><br></pre></td></tr></table></figure>
<pre><code>Text(1.6, 4, &#39;score:0.80&#39;)
</code></pre><img src="/2021/08/21/%E5%9B%9E%E5%BD%92%E5%A4%A7%E5%AE%B6%E6%97%8F/output_6_1.png" class="" alt="png">
<h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><p>中心思想：迭代的调整参数从而使损失函数最小化。</p>
<p>做法：通过测量参数向量$\theta$相关的误差函数和局部梯度，并沿着降低梯度的方向调整，直到梯度为0，到达最小值。</p>
<p>具体来说：首先使用一个随机的$\theta$值，然后逐步改进，每次踏出一步，每一步都尝试降低一点损失函数，直到算法收敛到一个最小值梯度下降中迭代的每一步的步长，叫做<strong>学习率</strong>，这是个超参数。</p>
<blockquote>
<p>如果学习率过高，会导致算法发散，值越来越大；<br>如果学习率太低，算法需要经过大量迭代才能收敛，这将耗费很长时间。</p>
</blockquote>
<p>梯度下降陷阱：在实际情况中，由于算法十分复杂，梯度有可能在未达到全局最小值时就不再下降，收敛到局部最小值在使用梯度下降时最好保证特征值的大小比例都差不多，对数据进行标准化或归一化后再进行训练。</p>
<h2 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h2><p>要实现梯度下降，需要计算每个模型关于参数$\theta_j$的损失函数的梯度，在计算梯度下降的每一步时，都是基于完整的训练集X的，这就是为什么该算法叫批量梯度下降。但是面对非常庞大的数据集时，算法会变的极慢。但是比标准方程或是SVD快的多。</p>
<p>有了梯度向量，我们就得到了梯度下降的公式，其中$\eta$是学习率：</p>
<script type="math/tex; mode=display">\theta_t=\theta_{t-1}-\eta \nabla_\theta MSE(\theta)</script><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>每一步在训练集中随机选择一个实例，并且仅基于该单个实例来计算梯度。</p>
<p>由于算法的随机性质，它比批量随机梯度下降要不规则的多，损失函数不再是缓缓降低直到最小值，而是不断上上下下，但总体来看还是缓慢下降直到收敛到最小值。当成本函数非常不规则时，随机梯度下降可以帮助算法跳出局部最小值。</p>
<p>随机性的好处是可以跳出局部最优，但缺点是永远定位不出最小值。要解决这个困难，一个好办法是逐步降低学习率，确定每个迭代学习率的函数叫做学习率调度。</p>
<blockquote>
<p>注意：使用随机梯度下降时，训练实例必须独立且均匀分布，以确保平均而言将参数拉向全局最优，确保该点可以对实例进行随机混洗。</p>
</blockquote>
<h2 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h2><p>在数据集中，随机取小型批量的实例集计算梯度。</p>
<h1 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h1><p>对于非线性的数据我们仍可以用线性模型进行拟合，我们先将每个特征的幂次方添加为一个新特征，然后再训练线性模型。</p>
<p>当存在多个特征时，<code>PolynomialFeatures</code>可以将特征的所有组合添加到给定的多项式阶数，但要注意特征组合的数量爆炸。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = <span class="number">6</span>*np.random.rand(<span class="number">100</span>,<span class="number">1</span>)-<span class="number">3</span></span><br><span class="line">y = <span class="number">0.5</span>*X**<span class="number">2</span>+X+<span class="number">2</span>+np.random.randn(<span class="number">100</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly = PolynomialFeatures(degree=<span class="number">2</span>,include_bias=<span class="literal">False</span>)</span><br><span class="line">X_poly = poly.fit_transform(X)</span><br><span class="line">X_poly[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([-2.82928969,  8.00488013])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">poly_lin_reg = LinearRegression()</span><br><span class="line">poly_lin_reg.fit(X_poly, y)</span><br><span class="line">poly_lin_reg.intercept_,poly_lin_reg.coef_</span><br></pre></td></tr></table></figure>
<pre><code>(array([2.27141448]), array([[0.98932735, 0.41514151]]))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_new = np.linspace(-<span class="number">3</span>,<span class="number">3</span>,<span class="number">100</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">y_new = poly_lin_reg.predict(poly.fit_transform(X_new))</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, c=<span class="string">&#x27;b&#x27;</span>,s=<span class="number">2</span>)</span><br><span class="line">plt.plot(X_new,y_new,c=<span class="string">&#x27;r&#x27;</span>)</span><br><span class="line">plt.text(<span class="number">1.6</span>,<span class="number">0</span>,<span class="string">&#x27;score:&#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>(poly_lin_reg.score(X_poly,y)))</span><br></pre></td></tr></table></figure>
<pre><code>Text(1.6, 0, &#39;score:0.84&#39;)
</code></pre><img src="/2021/08/21/%E5%9B%9E%E5%BD%92%E5%A4%A7%E5%AE%B6%E6%97%8F/output_16_1.png" class="" alt="png">
<h1 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h1><p>为了避免过拟合，有两种常用的方法：</p>
<ul>
<li>交叉验证：根据交叉验证的指标泛化较差而训练集上很好，说明过拟合；若两者的表现均不理想，则是欠拟合。</li>
<li>学习曲线；绘制模型再训练集和验证集上关于训练集大小（或训练迭代）的性能函数。</li>
</ul>
<p>欠拟合的学习曲线，训练误差和泛化误差都较高且最后都趋于平稳；过拟合时，泛化误差很高，而训练误差也会逐渐趋于平稳，其也较高。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_plot</span>(<span class="params">model,X,y</span>):</span></span><br><span class="line">    X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=<span class="number">0.2</span>)</span><br><span class="line">    train_errors, val_errors = [],[]</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(X_train)):</span><br><span class="line">        model.fit(X_train[:m], y_train[:m])</span><br><span class="line">        y_train_pred = model.predict(X_train[:m])</span><br><span class="line">        y_val_pred = model.predict(X_val)</span><br><span class="line">        train_errors.append(mean_squared_error(y_train[:m],y_train_pred))</span><br><span class="line">        val_errors.append(mean_squared_error(y_val,y_val_pred))</span><br><span class="line">    plt.plot(np.sqrt(train_errors),c=<span class="string">&#x27;b&#x27;</span>,label=<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    plt.plot(np.sqrt(val_errors),c=<span class="string">&#x27;r&#x27;</span>,label=<span class="string">&#x27;validation&#x27;</span>)</span><br><span class="line">    plt.ylim(<span class="number">0</span>,<span class="number">5</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lin = LinearRegression()</span><br><span class="line">poly_lin = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;poly_feature&#x27;</span>,PolynomialFeatures(degree=<span class="number">100</span>,include_bias=<span class="literal">False</span>)),</span><br><span class="line">    (<span class="string">&#x27;linear&#x27;</span>,LinearRegression())</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_plot(lin,X,y)</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E5%9B%9E%E5%BD%92%E5%A4%A7%E5%AE%B6%E6%97%8F/output_21_0.png" class="" alt="png">
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_plot(poly_lin,X,y)</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E5%9B%9E%E5%BD%92%E5%A4%A7%E5%AE%B6%E6%97%8F/output_22_0.png" class="" alt="png">
<h1 id="正则化线性模型"><a href="#正则化线性模型" class="headerlink" title="正则化线性模型"></a>正则化线性模型</h1><p>减少过拟合的好方法是对模型进行正则化（约束模型）：它拥有的自由度越少，过拟合数据的难度就越大。</p>
<h2 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h2><p>岭回归也称为Tikhonov正则化，是线性回归的正则化版本，将权重参数$\theta$的L2范数加入损失函数，迫使学习算法不仅拟合数据，还使模型权重尽可能的小。</p>
<blockquote>
<p>注意，尽在训练期间将正则化参数添加到损失函数中，训练完成后要使用非正则化的性能度量来评估模型的性能。<br>在执行岭回归或者其他正则化模型之前，都要对数据特征进行缩放，因为其对数据缩放特征敏感。</p>
</blockquote>
<p>岭回归的损失函数：</p>
<script type="math/tex; mode=display">J(\theta)=MSE(\theta)+\alpha \frac{1}{2}\sum_{i=1}^n\theta_i^2</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_ = np.linspace(<span class="number">0</span>,<span class="number">3</span>,<span class="number">100</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">plt.figure()</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">10</span>,<span class="number">100</span>]:</span><br><span class="line">    ridge = Ridge(alpha=alpha)</span><br><span class="line">    ridge.fit(X,y)</span><br><span class="line">    plt.scatter(X,y,c=<span class="string">&#x27;k&#x27;</span>,s=<span class="number">4</span>)</span><br><span class="line">    plt.plot(X_,ridge.predict(X_),label=<span class="string">&#x27;alpha=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(alpha))</span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E5%9B%9E%E5%BD%92%E5%A4%A7%E5%AE%B6%E6%97%8F/output_26_0.png" class="" alt="png">
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_ = np.linspace(<span class="number">0</span>,<span class="number">3</span>,<span class="number">100</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">plt.figure()</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1e-5</span>,<span class="number">1</span>]:</span><br><span class="line">    pipe = Pipeline([</span><br><span class="line">        (<span class="string">&#x27;0&#x27;</span>,PolynomialFeatures(degree=<span class="number">10</span>,include_bias=<span class="literal">False</span>)),</span><br><span class="line">        (<span class="string">&#x27;1&#x27;</span>,Ridge(alpha=alpha))</span><br><span class="line">    ])</span><br><span class="line">    pipe.fit(X,y)</span><br><span class="line">    plt.scatter(X,y,c=<span class="string">&#x27;k&#x27;</span>,s=<span class="number">4</span>)</span><br><span class="line">    plt.plot(X_,pipe.predict(X_),label=<span class="string">&#x27;alpha=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(alpha))</span><br><span class="line">    plt.ylim(<span class="number">0</span>,<span class="number">12</span>)</span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E5%9B%9E%E5%BD%92%E5%A4%A7%E5%AE%B6%E6%97%8F/output_27_0.png" class="" alt="png">
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> [<span class="number">0</span>,<span class="number">1e-5</span>,<span class="number">1</span>]:</span><br><span class="line">    sgd_reg = SGDRegressor(penalty=<span class="string">&#x27;l2&#x27;</span>,alpha=alpha)</span><br><span class="line">    sgd_reg.fit(X,y.ravel())</span><br><span class="line">    plt.scatter(X,y,c=<span class="string">&#x27;k&#x27;</span>,s=<span class="number">4</span>)</span><br><span class="line">    plt.plot(X_,sgd_reg.predict(X_),label=<span class="string">&#x27;alpha=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(sgd_reg.alpha))</span><br><span class="line">    plt.ylim(<span class="number">0</span>,<span class="number">12</span>)</span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E5%9B%9E%E5%BD%92%E5%A4%A7%E5%AE%B6%E6%97%8F/output_29_0.png" class="" alt="png">
<h2 id="Lasso回归"><a href="#Lasso回归" class="headerlink" title="Lasso回归"></a>Lasso回归</h2><p>线性回归的另一种正则化叫做最小绝对收缩和选择算子回归，简称Lasso回归。他向随时函数添加权重向量的L1范数。</p>
<p>Lasso回归的一个重要特点是，他倾向于完全消除最不重要特征的权重，会自动执行特征选择并输出一个稀疏模型。</p>
<p>其损失函数如下：</p>
<script type="math/tex; mode=display">J(\theta)=MSE(\theta)+\alpha \sum_{i=1}^n|\theta_i|</script><p>为避免在最优解附近反弹，需要逐渐降低训练期间的学习率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> [<span class="number">0.5</span>,<span class="number">1e-5</span>,<span class="number">1</span>]:</span><br><span class="line">    lasso_reg = Lasso(alpha=alpha)</span><br><span class="line">    lasso_reg.fit(X,y)</span><br><span class="line">    plt.scatter(X,y,c=<span class="string">&#x27;k&#x27;</span>,s=<span class="number">4</span>)</span><br><span class="line">    plt.plot(X_,lasso_reg.predict(X_),label=<span class="string">&#x27;alpha=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(lasso_reg.alpha))</span><br><span class="line">    plt.ylim(<span class="number">0</span>,<span class="number">12</span>)</span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E5%9B%9E%E5%BD%92%E5%A4%A7%E5%AE%B6%E6%97%8F/output_33_0.png" class="" alt="png">
<h2 id="弹性网络"><a href="#弹性网络" class="headerlink" title="弹性网络"></a>弹性网络</h2><p>弹性网络是介于岭回归和Lasso回归的中间地带，正则项是权重系数的L1,L2范数的混合，用混合系数r控制。<br>大多数情况下，我们应避免使用纯线性回归，默认岭回归；担当实际用到的特征数很少时，应倾向于使用Lasso或弹性网络。<br>弹性网络的损失函数如下</p>
<script type="math/tex; mode=display">J(\theta)=MSE(\theta)+r\alpha \sum_{i=1}^n|\theta_i|+\frac{1-r}{2}\alpha \sum_{i=1}^n\theta_i^2</script><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line"><span class="keyword">for</span> alpha <span class="keyword">in</span> [<span class="number">0.5</span>,<span class="number">1e-5</span>,<span class="number">1</span>]:</span><br><span class="line">    elatic = ElasticNet(alpha=alpha,l1_ratio=<span class="number">0.5</span>)</span><br><span class="line">    elatic.fit(X,y)</span><br><span class="line">    plt.scatter(X,y,c=<span class="string">&#x27;k&#x27;</span>,s=<span class="number">4</span>)</span><br><span class="line">    plt.plot(X_,elatic.predict(X_),label=<span class="string">&#x27;alpha=&#123;&#125;,r=&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(elatic.alpha,elatic.l1_ratio))</span><br><span class="line">    plt.ylim(<span class="number">0</span>,<span class="number">12</span>)</span><br><span class="line">    plt.legend()</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E5%9B%9E%E5%BD%92%E5%A4%A7%E5%AE%B6%E6%97%8F/output_36_0.png" class="" alt="png">
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>逻辑回归被广泛用于估算一个实例属于某个特定类别的概率，若预估概率超过50%啧判定该例属于该类别，称为正例，记为1；反之则不属于该类别，称为反类，记为0。它的输出是结果的数理逻辑值。</p>
<p>逻辑回归模型的估计概率：</p>
<p>$\hat{p}=h_\theta(\vec{x})=\sigma(\vec{x^T}\vec{\theta})$</p>
<p>逻辑回归预测模型：</p>
<p>$\hat{y}=<br>\begin{cases}<br>0&amp; \hat{p}&lt;0.5\\<br>1&amp; \hat{p}\ge0.5<br>\end{cases}$</p>
<p>单个训练实例的损失函数：</p>
<p>$c(\theta)=<br>\begin{cases}<br>-\log(\hat{p})&amp; y=1\\<br>-\log(1-\hat{p})&amp; y=0<br>\end{cases}$</p>
<p>逻辑回归损失函数：</p>
<p>$J(\theta)={1\over m} \sum_{i=1}^m[y^i\log(\hat{p}^i)+(1-y^i)\log(1-\hat{p}^i)]$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = load_iris()</span><br><span class="line">X = iris[<span class="string">&#x27;data&#x27;</span>][:,<span class="number">3</span>:]<span class="comment">#取得第四个特征的每个值</span></span><br><span class="line">y = (iris[<span class="string">&#x27;target&#x27;</span>]==<span class="number">2</span>).astype(np.<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris[<span class="string">&#x27;feature_names&#x27;</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;sepal length (cm)&#39;,
 &#39;sepal width (cm)&#39;,
 &#39;petal length (cm)&#39;,
 &#39;petal width (cm)&#39;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">log_reg = LogisticRegression()</span><br><span class="line">log_reg.fit(X,y)</span><br></pre></td></tr></table></figure>
<pre><code>LogisticRegression()
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_ = np.linspace(<span class="number">0</span>,<span class="number">3</span>,<span class="number">1000</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">y_ = log_reg.predict_proba(X_)<span class="comment">#predict()返回的是概率最大的类别，predict_proba()返回每种类别的可能性</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">5</span>))</span><br><span class="line">plt.plot(X_,y_[:,<span class="number">0</span>],<span class="string">&#x27;g--&#x27;</span>,label=<span class="string">&#x27;not virginica&#x27;</span>)</span><br><span class="line">plt.plot(X_,y_[:,<span class="number">1</span>],<span class="string">&#x27;b-&#x27;</span>,label=<span class="string">&#x27;virginica&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.plot(X[y==<span class="number">0</span>], y[y==<span class="number">0</span>], <span class="string">&quot;gs&quot;</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">1</span>], y[y==<span class="number">1</span>], <span class="string">&quot;b^&quot;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x1d311467520&gt;]
</code></pre><img src="/2021/08/21/%E5%9B%9E%E5%BD%92%E5%A4%A7%E5%AE%B6%E6%97%8F/output_43_1.png" class="" alt="png">
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">log_reg.predict([[<span class="number">2.3</span>],[<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>array([1, 0])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">log_reg.predict_proba([[<span class="number">2.3</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>array([[0.05889014, 0.94110986]])
</code></pre><h2 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h2><p>逻辑回归经过推广可以支持多个类别，称为softmax回归，或叫做多元逻辑回归。</p>
<p>给定一个实例x，Softmax回归模型首先计算出每个类k的分数，然后对这些分数应用softmax函数，估算出每个类的概率。</p>
<p>Softmax回归分类器一次只能预测一个类（多类一输出），因此他只能与互斥的类一同使用。</p>
<p>损失函数被称为交叉熵，用于衡量一组估算出的类的概率跟目标类的匹配程度。</p>
<p>Softmax函数：</p>
$\hat{p}_k={{\exp(s_k(\vec{x}))} \over {\sum_{j=1}^K\exp(s_j(\vec{x}))}}$
<p>回归分类预测：</p>
<p>$\hat{y}=argmax(\hat{p}_k)$</p>
<p>交叉熵损失函数：</p>
<p>$J(\theta)=-\frac{1}{m}\sum_{i=1}^m\sum_{k=1}^Ky_k^i\log(\hat{p}_k^i)$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = iris[<span class="string">&#x27;data&#x27;</span>][:,(<span class="number">2</span>,<span class="number">3</span>)]</span><br><span class="line">y = iris[<span class="string">&#x27;target&#x27;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">soft_reg = LogisticRegression(multi_class=<span class="string">&#x27;multinomial&#x27;</span>,solver=<span class="string">&#x27;lbfgs&#x27;</span>,C=<span class="number">10</span>)</span><br><span class="line">soft_reg.fit(X,y)</span><br></pre></td></tr></table></figure>
<pre><code>LogisticRegression(C=10, multi_class=&#39;multinomial&#39;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">soft_reg.predict([[<span class="number">5</span>,<span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>array([2])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">soft_reg.predict_proba([[<span class="number">5</span>,<span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x0,x1 = np.meshgrid(np.linspace(<span class="number">0</span>,<span class="number">8</span>,<span class="number">1000</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>),np.linspace(<span class="number">0</span>,<span class="number">3.5</span>,<span class="number">1000</span>).reshape(-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">X_ = np.c_[x0.ravel(),x1.ravel()]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_prob = soft_reg.predict_proba(X_)</span><br><span class="line">y_pred = soft_reg.predict(X_)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z=y_pred.reshape(x0.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot(X[y==<span class="number">2</span>, <span class="number">0</span>], X[y==<span class="number">2</span>, <span class="number">1</span>], <span class="string">&quot;g^&quot;</span>, label=<span class="string">&quot;Iris virginica&quot;</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>], <span class="string">&quot;bs&quot;</span>, label=<span class="string">&quot;Iris versicolor&quot;</span>)</span><br><span class="line">plt.plot(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>], <span class="string">&quot;yo&quot;</span>, label=<span class="string">&quot;Iris setosa&quot;</span>)</span><br><span class="line">plt.contour(x0,x1,z)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.axis([<span class="number">0</span>,<span class="number">8</span>,<span class="number">0</span>,<span class="number">3.5</span>])</span><br></pre></td></tr></table></figure>
<pre><code>(0.0, 8.0, 0.0, 3.5)
</code></pre><img src="/2021/08/21/%E5%9B%9E%E5%BD%92%E5%A4%A7%E5%AE%B6%E6%97%8F/output_54_1.png" class="" alt="png">
<h1 id="附录-参数列表"><a href="#附录-参数列表" class="headerlink" title="附录 参数列表"></a>附录 参数列表</h1><h2 id="LinearRegression"><a href="#LinearRegression" class="headerlink" title="LinearRegression"></a>LinearRegression</h2><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>normalize</td>
<td>bool, default=False:如果为True，则在回归之前通过减去均值并除以l2-范数来对回归变量X进行归一化。</td>
</tr>
<tr>
<td>n_jobs</td>
<td>int, default=None:设置用于计算的核心数。</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>coef_</td>
<td>线性回归问题的估计系数。</td>
</tr>
<tr>
<td>intercept_</td>
<td>线性模型中的截距项。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="ElasticNet"><a href="#ElasticNet" class="headerlink" title="ElasticNet"></a>ElasticNet</h2><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>alpha</td>
<td>float, default=1.0:乘以惩罚项的常数。</td>
</tr>
<tr>
<td>l1_ratio</td>
<td>float, default=0.5:弹性网络的混合系数。</td>
</tr>
<tr>
<td>max_iter</td>
<td>int, default=1000：最大迭代次数。</td>
</tr>
<tr>
<td>tol</td>
<td>float, default=1e-4：优化的容忍度。</td>
</tr>
<tr>
<td>warm_start</td>
<td>bool, default=False：设置为True时，重用前面调用的解决方案来进行初始化，否则，只清除前面的解决方案。</td>
</tr>
<tr>
<td>normalize</td>
<td>bool, default=False:如果为True，则在回归之前通过减去均值并除以l2-范数来对回归变量X进行归一化。</td>
</tr>
<tr>
<td>n_jobs</td>
<td>int, default=None:设置用于计算的核心数。</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>coef_</td>
<td>线性回归问题的估计系数。</td>
</tr>
<tr>
<td>intercept_</td>
<td>线性模型中的截距项。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Lasso"><a href="#Lasso" class="headerlink" title="Lasso"></a>Lasso</h2><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>normalize</td>
<td>bool, default=False:如果为True，则在回归之前通过减去均值并除以l2-范数来对回归变量X进行归一化。</td>
</tr>
<tr>
<td>alpha</td>
<td>float, default=1.0:乘以惩罚项的常数。</td>
</tr>
<tr>
<td>n_jobs</td>
<td>int, default=None:设置用于计算的核心数。</td>
</tr>
<tr>
<td>max_iter</td>
<td>int, default=1000：最大迭代次数。</td>
</tr>
<tr>
<td>tol</td>
<td>float, default=1e-4：优化的容忍度。</td>
</tr>
<tr>
<td>warm_start</td>
<td>bool, default=False：设置为True时，重用前面调用的解决方案来进行初始化，否则，只清除前面的解决方案。</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>coef_</td>
<td>线性回归问题的估计系数。</td>
</tr>
<tr>
<td>intercept_</td>
<td>线性模型中的截距项。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="Ridge"><a href="#Ridge" class="headerlink" title="Ridge"></a>Ridge</h2><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>normalize</td>
<td>bool, default=False:如果为True，则在回归之前通过减去均值并除以l2-范数来对回归变量X进行归一化。</td>
</tr>
<tr>
<td>alpha</td>
<td>float, default=1.0:乘以惩罚项的常数。</td>
</tr>
<tr>
<td>n_jobs</td>
<td>int, default=None:设置用于计算的核心数。</td>
</tr>
<tr>
<td>max_iter</td>
<td>int, default=1000：最大迭代次数。</td>
</tr>
<tr>
<td>tol</td>
<td>float, default=1e-4：优化的容忍度。</td>
</tr>
<tr>
<td>warm_start</td>
<td>bool, default=False：设置为True时，重用前面调用的解决方案来进行初始化，否则，只清除前面的解决方案。</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>coef_</td>
<td>线性回归问题的估计系数。</td>
</tr>
<tr>
<td>intercept_</td>
<td>线性模型中的截距项。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="LogisticRegression"><a href="#LogisticRegression" class="headerlink" title="LogisticRegression"></a>LogisticRegression</h2><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>penalty</td>
<td>{‘L1’, ‘L2’, ‘elasticnet’, ‘none’}, default=’L2’:用于指定处罚中使用的规范。</td>
</tr>
<tr>
<td>C</td>
<td>float, default=1.0:正则强度的倒数；必须为正浮点数。</td>
</tr>
<tr>
<td>n_jobs</td>
<td>int, default=None:设置用于计算的核心数。</td>
</tr>
<tr>
<td>max_iter</td>
<td>int, default=1000：最大迭代次数。</td>
</tr>
<tr>
<td>tol</td>
<td>float, default=1e-4：优化的容忍度。</td>
</tr>
<tr>
<td>warm_start</td>
<td>bool, default=False：设置为True时，重用前面调用的解决方案来进行初始化，否则，只清除前面的解决方案。</td>
</tr>
<tr>
<td>class_weight</td>
<td>dict or ‘balanced’, default=None:以{class_label: weight}的形式与类别关联的权重。如果没有给出，所有类别的权重都应该是1。<br>“balanced”模式使用y的值来自动调整为与输入数据中的类频率成反比的权重。</td>
</tr>
<tr>
<td>multi_class</td>
<td>{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’:如果选择的选项是“ ovr”，则每个标签都看做二分类问题。对于“multinomial”，即使数据是二分类的，损失最小是多项式损失拟合整个概率分布。</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>coef_</td>
<td>线性回归问题的估计系数。</td>
</tr>
<tr>
<td>intercept_</td>
<td>线性模型中的截距项。</td>
</tr>
<tr>
<td>classes_</td>
<td>分类器已知的类别标签列表。</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <tags>
        <tag>tensorflow</tag>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title>使用Keras构建神经网络</title>
    <url>/2021/08/21/%E4%BD%BF%E7%94%A8Keras%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h1 id="使用顺序API构建图像分类器"><a href="#使用顺序API构建图像分类器" class="headerlink" title="使用顺序API构建图像分类器"></a>使用顺序API构建图像分类器</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fashion_mnist = keras.datasets.fashion_mnist</span><br><span class="line">(X_train_full,y_train_full),(X_test,y_test) = fashion_mnist.load_data()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train_full.shape,X_train_full.dtype</span><br></pre></td></tr></table></figure>
<pre><code>((60000, 28, 28), dtype(&#39;uint8&#39;))
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_valid, X_train = X_train_full[:<span class="number">5000</span>]/<span class="number">255.0</span>, X_train_full[<span class="number">5000</span>:]/<span class="number">255.0</span></span><br><span class="line">y_valid, y_train = y_train_full[:<span class="number">5000</span>], y_train_full[<span class="number">5000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">class_names = [<span class="string">&quot;T-shirt/top&quot;</span>, <span class="string">&quot;Trouser&quot;</span>, <span class="string">&quot;Pullover&quot;</span>, <span class="string">&quot;Dress&quot;</span>, <span class="string">&quot;Coat&quot;</span>, <span class="string">&quot;Sandal&quot;</span>, <span class="string">&quot;Shirt&quot;</span>, <span class="string">&quot;Sneaker&quot;</span>, <span class="string">&quot;Bag&quot;</span>, <span class="string">&quot;Ankle boot&quot;</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">5</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(X_train[i],cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.title(class_names[y_train[i]])</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E4%BD%BF%E7%94%A8Keras%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/output_6_0.png" class="" alt="png">
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = keras.Sequential([keras.layers.Flatten(input_shape=[<span class="number">28</span>,<span class="number">28</span>]),</span><br><span class="line">                          keras.layers.Dense(<span class="number">300</span>,activation=keras.activations.relu),</span><br><span class="line">                          keras.layers.Dense(<span class="number">100</span>,activation=keras.activations.relu),</span><br><span class="line">                          keras.layers.Dense(<span class="number">10</span>,activation=keras.activations.softmax)])</span><br></pre></td></tr></table></figure>
<p>初始化时，对于每一层的权重和偏置可以使用<code>kernel_initializer</code>或<code>bias_initializer</code>进行初始化；\<br>权重矩阵的形状取决于输入的个数，我们应在第一层中指定<code>input_shape</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten (Flatten)            (None, 784)               0         
_________________________________________________________________
dense (Dense)                (None, 300)               235500    
_________________________________________________________________
dense_1 (Dense)              (None, 100)               30100     
_________________________________________________________________
dense_2 (Dense)              (None, 10)                1010      
=================================================================
Total params: 266,610
Trainable params: 266,610
Non-trainable params: 0
_________________________________________________________________
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.layers</span><br></pre></td></tr></table></figure>
<pre><code>[&lt;keras.layers.core.Flatten at 0x294b10e2b50&gt;,
 &lt;keras.layers.core.Dense at 0x294b1182a90&gt;,
 &lt;keras.layers.core.Dense at 0x294b1141e20&gt;,
 &lt;keras.layers.core.Dense at 0x294b11817f0&gt;]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.layers[<span class="number">1</span>] == model.get_layer(<span class="string">&#x27;dense&#x27;</span>)</span><br></pre></td></tr></table></figure>
<pre><code>True
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weights, bias = model.layers[<span class="number">1</span>].get_weights()<span class="comment">#also has set_weights() or use kernel_initializer and bias_initializer</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weights</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 0.00432374,  0.069911  ,  0.04554318, ...,  0.02112412,
        -0.05035692, -0.04687664],
       [-0.02890603,  0.06612819,  0.05927478, ..., -0.04826843,
         0.00918571, -0.03757042],
       [-0.0468495 , -0.05732308,  0.00990029, ..., -0.00668625,
        -0.00170599,  0.01458585],
       ...,
       [-0.03658697, -0.03046996,  0.06218758, ...,  0.07221685,
        -0.03757465,  0.06432179],
       [ 0.02678267, -0.00753582,  0.02744661, ...,  0.07104121,
        -0.02597448,  0.04335685],
       [ 0.02518395,  0.05691384,  0.00831134, ..., -0.05676897,
         0.04779277, -0.06324223]], dtype=float32)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bias</span><br></pre></td></tr></table></figure>
<pre><code>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=keras.losses.sparse_categorical_crossentropy,</span><br><span class="line">              optimizer=keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>),</span><br><span class="line">              metrics=keras.metrics.sparse_categorical_accuracy)</span><br></pre></td></tr></table></figure>
<p>注意：我们使用的损失函数如上所示，因为我们具有稀疏标签（即对于每个实例，只有一个目标类索引，例子情况下为0~9），并且这些类都是互斥的，\<br>若每个实例的每个类都有一个目标概率（独热编码），则我们要使用<code>categorical_crossentropy</code>。\<br>若我们正在执行二进制分类，输出层中应使用<code>sigmoid</code>激活函数，且使用<code>binary_crossentropy</code>损失。\<br>如果要将稀疏标签转换为独热向量，可以使用<code>keras.utils.to_categorical()</code>，反之则使用<code>np.argmax(axis=1)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit(X_train,y_train,epochs=<span class="number">30</span>,validation_data=(X_valid,y_valid))</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/30
1719/1719 [==============================] - 12s 5ms/step - loss: 0.7298 - sparse_categorical_accuracy: 0.7560 - val_loss: 0.5163 - val_sparse_categorical_accuracy: 0.8270
Epoch 2/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.4916 - sparse_categorical_accuracy: 0.8287 - val_loss: 0.4473 - val_sparse_categorical_accuracy: 0.8508
Epoch 3/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.4459 - sparse_categorical_accuracy: 0.8434 - val_loss: 0.4199 - val_sparse_categorical_accuracy: 0.8638
Epoch 4/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.4185 - sparse_categorical_accuracy: 0.8523 - val_loss: 0.4072 - val_sparse_categorical_accuracy: 0.8606
Epoch 5/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.3985 - sparse_categorical_accuracy: 0.8591 - val_loss: 0.4054 - val_sparse_categorical_accuracy: 0.8636
Epoch 6/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.3830 - sparse_categorical_accuracy: 0.8640 - val_loss: 0.3741 - val_sparse_categorical_accuracy: 0.8730
Epoch 7/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.3692 - sparse_categorical_accuracy: 0.8692 - val_loss: 0.3805 - val_sparse_categorical_accuracy: 0.8696
Epoch 8/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.3576 - sparse_categorical_accuracy: 0.8736 - val_loss: 0.3639 - val_sparse_categorical_accuracy: 0.8738
Epoch 9/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.3472 - sparse_categorical_accuracy: 0.8769 - val_loss: 0.3516 - val_sparse_categorical_accuracy: 0.8794
Epoch 10/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.3377 - sparse_categorical_accuracy: 0.8808 - val_loss: 0.3416 - val_sparse_categorical_accuracy: 0.8830
Epoch 11/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.3295 - sparse_categorical_accuracy: 0.8828 - val_loss: 0.3500 - val_sparse_categorical_accuracy: 0.8788
Epoch 12/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.3198 - sparse_categorical_accuracy: 0.8847 - val_loss: 0.3294 - val_sparse_categorical_accuracy: 0.8858
Epoch 13/30
1719/1719 [==============================] - 9s 5ms/step - loss: 0.3128 - sparse_categorical_accuracy: 0.8881 - val_loss: 0.3449 - val_sparse_categorical_accuracy: 0.8784
Epoch 14/30
1719/1719 [==============================] - 8s 4ms/step - loss: 0.3050 - sparse_categorical_accuracy: 0.8897 - val_loss: 0.3874 - val_sparse_categorical_accuracy: 0.8580
Epoch 15/30
1719/1719 [==============================] - 8s 4ms/step - loss: 0.2989 - sparse_categorical_accuracy: 0.8931 - val_loss: 0.3280 - val_sparse_categorical_accuracy: 0.8854
Epoch 16/30
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2927 - sparse_categorical_accuracy: 0.8942 - val_loss: 0.3285 - val_sparse_categorical_accuracy: 0.8794
Epoch 17/30
1719/1719 [==============================] - 9s 5ms/step - loss: 0.2864 - sparse_categorical_accuracy: 0.8969 - val_loss: 0.3190 - val_sparse_categorical_accuracy: 0.8886
Epoch 18/30
1719/1719 [==============================] - 8s 4ms/step - loss: 0.2810 - sparse_categorical_accuracy: 0.8994 - val_loss: 0.3130 - val_sparse_categorical_accuracy: 0.8898
Epoch 19/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.2751 - sparse_categorical_accuracy: 0.9013 - val_loss: 0.3186 - val_sparse_categorical_accuracy: 0.8886
Epoch 20/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.2702 - sparse_categorical_accuracy: 0.9030 - val_loss: 0.3252 - val_sparse_categorical_accuracy: 0.8842
Epoch 21/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.2643 - sparse_categorical_accuracy: 0.9039 - val_loss: 0.3085 - val_sparse_categorical_accuracy: 0.8898
Epoch 22/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.2609 - sparse_categorical_accuracy: 0.9064 - val_loss: 0.3574 - val_sparse_categorical_accuracy: 0.8664
Epoch 23/30
1719/1719 [==============================] - 6s 4ms/step - loss: 0.2561 - sparse_categorical_accuracy: 0.9074 - val_loss: 0.3047 - val_sparse_categorical_accuracy: 0.8922
Epoch 24/30
1719/1719 [==============================] - 6s 4ms/step - loss: 0.2507 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.3185 - val_sparse_categorical_accuracy: 0.8862
Epoch 25/30
1719/1719 [==============================] - 8s 5ms/step - loss: 0.2471 - sparse_categorical_accuracy: 0.9112 - val_loss: 0.3100 - val_sparse_categorical_accuracy: 0.8896
Epoch 26/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.2427 - sparse_categorical_accuracy: 0.9133 - val_loss: 0.3052 - val_sparse_categorical_accuracy: 0.8894
Epoch 27/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.2396 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.3033 - val_sparse_categorical_accuracy: 0.8934
Epoch 28/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.2341 - sparse_categorical_accuracy: 0.9161 - val_loss: 0.3121 - val_sparse_categorical_accuracy: 0.8874
Epoch 29/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.2310 - sparse_categorical_accuracy: 0.9171 - val_loss: 0.3057 - val_sparse_categorical_accuracy: 0.8888
Epoch 30/30
1719/1719 [==============================] - 7s 4ms/step - loss: 0.2272 - sparse_categorical_accuracy: 0.9190 - val_loss: 0.2938 - val_sparse_categorical_accuracy: 0.8928
</code></pre><p>我们可以设置<code>validation_split</code>指定用于验证的训练集的比例；\<br>如果训练集非常不平衡，某些类过多其他类不足，可以设置<code>class_weight</code>给代表性不足的类更大的权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history.params</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;verbose&#39;: 1, &#39;epochs&#39;: 30, &#39;steps&#39;: 1719&#125;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.DataFrame(history.history).plot(figsize=(<span class="number">10</span>,<span class="number">8</span>))</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E4%BD%BF%E7%94%A8Keras%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/output_20_0.png" class="" alt="png">
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.evaluate(X_test,y_test,batch_size=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>
<pre><code>1/1 [==============================] - 0s 38ms/step - loss: 61.2780 - sparse_categorical_accuracy: 0.8535





[61.278011322021484, 0.8535000085830688]
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_new = X_test[:<span class="number">3</span>]</span><br><span class="line">y_pred = model.predict(X_new).argmax(axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    plt.subplot(<span class="number">1</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(X_new[i],cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.title(class_names[y_pred[i]])</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E4%BD%BF%E7%94%A8Keras%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/output_23_0.png" class="" alt="png">
<h1 id="使用顺序API构建回归MLP"><a href="#使用顺序API构建回归MLP" class="headerlink" title="使用顺序API构建回归MLP"></a>使用顺序API构建回归MLP</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_california_housing</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">housing = fetch_california_housing()</span><br><span class="line"></span><br><span class="line">X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=<span class="number">42</span>)</span><br><span class="line">X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">scaler = StandardScaler()</span><br><span class="line">X_train = scaler.fit_transform(X_train)</span><br><span class="line">X_valid = scaler.transform(X_valid)</span><br><span class="line">X_test = scaler.transform(X_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = keras.models.Sequential([</span><br><span class="line">    keras.layers.Dense(<span class="number">30</span>, activation=<span class="string">&quot;relu&quot;</span>, input_shape=X_train.shape[<span class="number">1</span>:]),</span><br><span class="line">    keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line">])</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&quot;mean_squared_error&quot;</span>, optimizer=keras.optimizers.SGD(lr=<span class="number">1e-3</span>))</span><br><span class="line">history = model.fit(X_train, y_train, epochs=<span class="number">20</span>, validation_data=(X_valid, y_valid))</span><br><span class="line">mse_test = model.evaluate(X_test, y_test)</span><br><span class="line">X_new = X_test[:<span class="number">3</span>]</span><br><span class="line">y_pred = model.predict(X_new)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
363/363 [==============================] - 2s 6ms/step - loss: 2.6348 - val_loss: 1.4654
Epoch 2/20
363/363 [==============================] - 2s 5ms/step - loss: 0.8950 - val_loss: 0.8080
Epoch 3/20
363/363 [==============================] - 2s 5ms/step - loss: 0.7566 - val_loss: 0.6868
Epoch 4/20
363/363 [==============================] - 2s 5ms/step - loss: 0.7088 - val_loss: 0.7151
Epoch 5/20
363/363 [==============================] - 2s 4ms/step - loss: 0.6756 - val_loss: 0.6320
Epoch 6/20
363/363 [==============================] - 1s 4ms/step - loss: 0.6457 - val_loss: 0.6111
Epoch 7/20
363/363 [==============================] - 1s 4ms/step - loss: 0.6187 - val_loss: 0.5899
Epoch 8/20
363/363 [==============================] - 1s 4ms/step - loss: 0.5930 - val_loss: 0.6116
Epoch 9/20
363/363 [==============================] - 1s 4ms/step - loss: 0.5697 - val_loss: 0.6095
Epoch 10/20
363/363 [==============================] - 1s 4ms/step - loss: 0.5487 - val_loss: 0.5161
Epoch 11/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5286 - val_loss: 0.4910
Epoch 12/20
363/363 [==============================] - 1s 4ms/step - loss: 0.5115 - val_loss: 0.4832
Epoch 13/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4972 - val_loss: 0.4574
Epoch 14/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4849 - val_loss: 0.4465
Epoch 15/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4748 - val_loss: 0.4355
Epoch 16/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4662 - val_loss: 0.4291
Epoch 17/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4587 - val_loss: 0.4225
Epoch 18/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4524 - val_loss: 0.4190
Epoch 19/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4469 - val_loss: 0.4179
Epoch 20/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4419 - val_loss: 0.4159
162/162 [==============================] - 0s 2ms/step - loss: 0.4366
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(pd.DataFrame(history.history))</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.gca().set_ylim(<span class="number">0</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<pre><code>(0.0, 1.0)
</code></pre><img src="/2021/08/21/%E4%BD%BF%E7%94%A8Keras%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/output_27_1.png" class="" alt="png">
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_pred</span><br></pre></td></tr></table></figure>
<pre><code>array([[0.6820665],
       [1.432928 ],
       [3.2171566]], dtype=float32)
</code></pre><h1 id="使用函数式API构建复杂模型"><a href="#使用函数式API构建复杂模型" class="headerlink" title="使用函数式API构建复杂模型"></a>使用函数式API构建复杂模型</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_ = keras.layers.Input(shape=X_train.shape[<span class="number">1</span>:])</span><br><span class="line">hidden1 = keras.layers.Dense(<span class="number">30</span>,activation=<span class="string">&#x27;relu&#x27;</span>)(input_)</span><br><span class="line">hidden2 = keras.layers.Dense(<span class="number">30</span>,activation=<span class="string">&#x27;relu&#x27;</span>)(hidden1)</span><br><span class="line">concat = keras.layers.Concatenate()([input_,hidden2])</span><br><span class="line">output = keras.layers.Dense(<span class="number">1</span>)(concat)</span><br><span class="line">model = keras.Model(inputs=input_,outputs=output)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;model_1&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_2 (InputLayer)            [(None, 8)]          0                                            
__________________________________________________________________________________________________
dense_8 (Dense)                 (None, 30)           270         input_2[0][0]                    
__________________________________________________________________________________________________
dense_9 (Dense)                 (None, 30)           930         dense_8[0][0]                    
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 38)           0           input_2[0][0]                    
                                                                 dense_9[0][0]                    
__________________________________________________________________________________________________
dense_10 (Dense)                (None, 1)            39          concatenate_1[0][0]              
==================================================================================================
Total params: 1,239
Trainable params: 1,239
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.utils.plot_model(model,show_shapes=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E4%BD%BF%E7%94%A8Keras%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/output_32_0.png" class="" alt="png">
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_A = keras.layers.Input(shape=[<span class="number">5</span>],name=<span class="string">&#x27;wide_input&#x27;</span>)</span><br><span class="line">input_B = keras.layers.Input(shape=[<span class="number">6</span>],name=<span class="string">&#x27;deep_input&#x27;</span>)</span><br><span class="line">hidden1 = keras.layers.Dense(<span class="number">30</span>,activation=<span class="string">&#x27;relu&#x27;</span>)(input_B)</span><br><span class="line">hidden2 = keras.layers.Dense(<span class="number">30</span>,activation=<span class="string">&#x27;relu&#x27;</span>)(hidden1)</span><br><span class="line">concat = keras.layers.Concatenate()([input_A,hidden2])</span><br><span class="line">output = keras.layers.Dense(<span class="number">1</span>,name=<span class="string">&#x27;output&#x27;</span>)(concat)</span><br><span class="line">model = keras.Model(inputs=[input_A,input_B],outputs=[output])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;model&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
deep_input (InputLayer)         [(None, 6)]          0                                            
__________________________________________________________________________________________________
dense (Dense)                   (None, 30)           210         deep_input[0][0]                 
__________________________________________________________________________________________________
wide_input (InputLayer)         [(None, 5)]          0                                            
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 30)           930         dense[0][0]                      
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 35)           0           wide_input[0][0]                 
                                                                 dense_1[0][0]                    
__________________________________________________________________________________________________
output (Dense)                  (None, 1)            36          concatenate[0][0]                
==================================================================================================
Total params: 1,176
Trainable params: 1,176
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.utils.plot_model(model,show_shapes=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E4%BD%BF%E7%94%A8Keras%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/output_36_0.png" class="" alt="png">
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mse&#x27;</span>,optimizer=keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>))</span><br></pre></td></tr></table></figure>
<p>注意：在创建模型时，我们制定了多个输入，我们在<code>fit</code>时就应该传入多个输入，在<code>predict</code>,<code>evaluate</code>时都应该有多个输入。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train_A, X_train_B = X_train[:,:<span class="number">5</span>], X_train[:,<span class="number">2</span>:]</span><br><span class="line">X_valid_A, X_valid_B = X_valid[:,:<span class="number">5</span>], X_valid[:,<span class="number">2</span>:]</span><br><span class="line">X_test_A, X_test_B = X_test[:,:<span class="number">5</span>], X_test[:,<span class="number">2</span>:]</span><br><span class="line">X_new_A, X_new_B = X_test_A[:<span class="number">3</span>], X_test_B[:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit((X_train_A,X_train_B),y_train,validation_data=((X_valid_A,X_valid_B),y_valid),epochs=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
363/363 [==============================] - 2s 4ms/step - loss: 2.6025 - val_loss: 5.7793
Epoch 2/20
363/363 [==============================] - 1s 4ms/step - loss: 0.8397 - val_loss: 2.4269
Epoch 3/20
363/363 [==============================] - 1s 4ms/step - loss: 0.6759 - val_loss: 1.3355
Epoch 4/20
363/363 [==============================] - 1s 3ms/step - loss: 0.6015 - val_loss: 0.9239
Epoch 5/20
363/363 [==============================] - 1s 4ms/step - loss: 0.5522 - val_loss: 0.6919
Epoch 6/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5167 - val_loss: 0.5866
Epoch 7/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4893 - val_loss: 0.5162
Epoch 8/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4697 - val_loss: 0.4804
Epoch 9/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4546 - val_loss: 0.4506
Epoch 10/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4439 - val_loss: 0.4349
Epoch 11/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4360 - val_loss: 0.4267
Epoch 12/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4299 - val_loss: 0.4200
Epoch 13/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4259 - val_loss: 0.4198
Epoch 14/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4217 - val_loss: 0.4181
Epoch 15/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4196 - val_loss: 0.4144
Epoch 16/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4168 - val_loss: 0.4156
Epoch 17/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4147 - val_loss: 0.4136
Epoch 18/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4127 - val_loss: 0.4101
Epoch 19/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4111 - val_loss: 0.4070
Epoch 20/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4094 - val_loss: 0.4078
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.evaluate((X_test_A,X_test_B),y_test)</span><br></pre></td></tr></table></figure>
<pre><code>162/162 [==============================] - 0s 2ms/step - loss: 0.4078





0.40784752368927
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.predict((X_new_A,X_new_B))</span><br></pre></td></tr></table></figure>
<pre><code>array([[0.5327476],
       [1.9329988],
       [3.3600936]], dtype=float32)
</code></pre><hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_A = keras.layers.Input(shape=[<span class="number">5</span>],name=<span class="string">&#x27;wide_input&#x27;</span>)</span><br><span class="line">input_B = keras.layers.Input(shape=[<span class="number">6</span>],name=<span class="string">&#x27;deep_input&#x27;</span>)</span><br><span class="line">hidden1 = keras.layers.Dense(<span class="number">30</span>,activation=<span class="string">&#x27;relu&#x27;</span>)(input_B)</span><br><span class="line">hidden2 = keras.layers.Dense(<span class="number">30</span>,activation=<span class="string">&#x27;relu&#x27;</span>)(hidden1)</span><br><span class="line">concat = keras.layers.Concatenate()([input_A,hidden2])</span><br><span class="line">output1 = keras.layers.Dense(<span class="number">1</span>,name=<span class="string">&#x27;output&#x27;</span>)(concat)</span><br><span class="line">output2 = keras.layers.Dense(<span class="number">1</span>,name=<span class="string">&#x27;aux&#x27;</span>)(hidden2)</span><br><span class="line">model = keras.Model(inputs=[input_A,input_B],outputs=[output1,output2])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<pre><code>Model: &quot;model_1&quot;
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
deep_input (InputLayer)         [(None, 6)]          0                                            
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 30)           210         deep_input[0][0]                 
__________________________________________________________________________________________________
wide_input (InputLayer)         [(None, 5)]          0                                            
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 30)           930         dense_2[0][0]                    
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 35)           0           wide_input[0][0]                 
                                                                 dense_3[0][0]                    
__________________________________________________________________________________________________
output (Dense)                  (None, 1)            36          concatenate_1[0][0]              
__________________________________________________________________________________________________
aux (Dense)                     (None, 1)            31          dense_3[0][0]                    
==================================================================================================
Total params: 1,207
Trainable params: 1,207
Non-trainable params: 0
__________________________________________________________________________________________________
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras.utils.plot_model(model,show_shapes=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<img src="/2021/08/21/%E4%BD%BF%E7%94%A8Keras%E6%9E%84%E5%BB%BA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/output_46_0.png" class="" alt="png">
<p>注意：每个输出都要有自己的损失函数，所以当我们编译模型时应该传递一系列的损失（如果传递单个损失，则默认所有输出使用相同的损失）。</p>
<p>默认情况下，Keras计算所有这些损失，并简单的相加得到最终的训练损失；而我们关注的是主要输出，所以要给主要输出更大的权重，我们可以通过设置参数<code>loss_weights</code>来指定不同损失的权重。除此之外，我们<code>fit</code>,<code>evaluate</code>时也要传入多个用于验证的数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=[<span class="string">&#x27;mse&#x27;</span>,<span class="string">&#x27;mse&#x27;</span>],loss_weights=[<span class="number">0.9</span>,<span class="number">0.1</span>],optimizer=<span class="string">&#x27;sgd&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model.fit([X_train_A,X_train_B],[y_train,y_train],epochs=<span class="number">20</span>,validation_data=([X_valid_A,X_valid_B],[y_valid,y_valid]))</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
363/363 [==============================] - 2s 6ms/step - loss: 0.5364 - output_loss: 0.4733 - aux_loss: 1.1042 - val_loss: 2.4807 - val_output_loss: 2.6077 - val_aux_loss: 1.3375
Epoch 2/20
363/363 [==============================] - 2s 5ms/step - loss: 0.5027 - output_loss: 0.4567 - aux_loss: 0.9175 - val_loss: 1.3039 - val_output_loss: 1.3301 - val_aux_loss: 1.0683
Epoch 3/20
363/363 [==============================] - 2s 5ms/step - loss: 0.4725 - output_loss: 0.4361 - aux_loss: 0.8008 - val_loss: 0.9919 - val_output_loss: 0.9801 - val_aux_loss: 1.0980
Epoch 4/20
363/363 [==============================] - 2s 5ms/step - loss: 0.4479 - output_loss: 0.4186 - aux_loss: 0.7117 - val_loss: 0.4309 - val_output_loss: 0.4022 - val_aux_loss: 0.6895
Epoch 5/20
363/363 [==============================] - 2s 5ms/step - loss: 0.4334 - output_loss: 0.4089 - aux_loss: 0.6546 - val_loss: 0.4091 - val_output_loss: 0.3853 - val_aux_loss: 0.6233
Epoch 6/20
363/363 [==============================] - 2s 5ms/step - loss: 0.4208 - output_loss: 0.3989 - aux_loss: 0.6178 - val_loss: 0.4338 - val_output_loss: 0.4149 - val_aux_loss: 0.6042
Epoch 7/20
363/363 [==============================] - 2s 5ms/step - loss: 0.4143 - output_loss: 0.3942 - aux_loss: 0.5945 - val_loss: 0.4099 - val_output_loss: 0.3919 - val_aux_loss: 0.5717
Epoch 8/20
363/363 [==============================] - 2s 5ms/step - loss: 0.4081 - output_loss: 0.3895 - aux_loss: 0.5754 - val_loss: 0.3863 - val_output_loss: 0.3680 - val_aux_loss: 0.5504
Epoch 9/20
363/363 [==============================] - 2s 5ms/step - loss: 0.4041 - output_loss: 0.3867 - aux_loss: 0.5606 - val_loss: 0.3876 - val_output_loss: 0.3707 - val_aux_loss: 0.5395
Epoch 10/20
363/363 [==============================] - 2s 5ms/step - loss: 0.3987 - output_loss: 0.3821 - aux_loss: 0.5485 - val_loss: 0.3737 - val_output_loss: 0.3570 - val_aux_loss: 0.5239
Epoch 11/20
363/363 [==============================] - 2s 5ms/step - loss: 0.3933 - output_loss: 0.3776 - aux_loss: 0.5348 - val_loss: 0.3769 - val_output_loss: 0.3604 - val_aux_loss: 0.5250
Epoch 12/20
363/363 [==============================] - 2s 5ms/step - loss: 0.3951 - output_loss: 0.3802 - aux_loss: 0.5291 - val_loss: 0.3697 - val_output_loss: 0.3540 - val_aux_loss: 0.5103
Epoch 13/20
363/363 [==============================] - 2s 5ms/step - loss: 0.3882 - output_loss: 0.3738 - aux_loss: 0.5178 - val_loss: 0.3646 - val_output_loss: 0.3488 - val_aux_loss: 0.5064
Epoch 14/20
363/363 [==============================] - 2s 5ms/step - loss: 0.3843 - output_loss: 0.3702 - aux_loss: 0.5115 - val_loss: 0.3543 - val_output_loss: 0.3388 - val_aux_loss: 0.4932
Epoch 15/20
363/363 [==============================] - 2s 5ms/step - loss: 0.3813 - output_loss: 0.3680 - aux_loss: 0.5011 - val_loss: 0.3764 - val_output_loss: 0.3620 - val_aux_loss: 0.5061
Epoch 16/20
363/363 [==============================] - 2s 5ms/step - loss: 0.3886 - output_loss: 0.3752 - aux_loss: 0.5085 - val_loss: 0.6404 - val_output_loss: 0.6423 - val_aux_loss: 0.6231
Epoch 17/20
363/363 [==============================] - 2s 5ms/step - loss: 0.3806 - output_loss: 0.3679 - aux_loss: 0.4952 - val_loss: 0.3480 - val_output_loss: 0.3344 - val_aux_loss: 0.4705
Epoch 18/20
363/363 [==============================] - 2s 5ms/step - loss: 0.3703 - output_loss: 0.3575 - aux_loss: 0.4855 - val_loss: 0.3608 - val_output_loss: 0.3481 - val_aux_loss: 0.4753
Epoch 19/20
363/363 [==============================] - 2s 5ms/step - loss: 0.3666 - output_loss: 0.3543 - aux_loss: 0.4766 - val_loss: 0.3572 - val_output_loss: 0.3452 - val_aux_loss: 0.4650
Epoch 20/20
363/363 [==============================] - 2s 5ms/step - loss: 0.3699 - output_loss: 0.3584 - aux_loss: 0.4740 - val_loss: 0.3714 - val_output_loss: 0.3610 - val_aux_loss: 0.4657
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">total_loss, main_loss, aux_loss = model.evaluate([X_test_A,X_test_B],[y_test,y_test])</span><br><span class="line">y_pred_main, y_aux = model.predict([X_new_A,X_new_B])</span><br></pre></td></tr></table></figure>
<pre><code>162/162 [==============================] - 0s 3ms/step - loss: 0.3888 - output_loss: 0.3806 - aux_loss: 0.4621
</code></pre><h1 id="使用子类API构建动态模型"><a href="#使用子类API构建动态模型" class="headerlink" title="使用子类API构建动态模型"></a>使用子类API构建动态模型</h1><p>通过对<code>keras.Model</code>继承，在构造函数中创建所需要的层，再在<code>call</code>方法中执行所需的计算即可。但这样Keras无法对其进行检查。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myModel</span>(<span class="params">keras.Model</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,units=<span class="number">30</span>,activation=<span class="string">&#x27;relu&#x27;</span>,**kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(**kwargs)</span><br><span class="line">        self.hidden1 = keras.layers.Dense(units,activation=activation)</span><br><span class="line">        self.hidden2 = keras.layers.Dense(units,activation=activation)</span><br><span class="line">        self.main_output = keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line">        self.aux = keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span>(<span class="params">self,inputs</span>):</span></span><br><span class="line">        input_A, input_B = inputs</span><br><span class="line">        hidden1 = self.hidden1(input_B)</span><br><span class="line">        hidden2 = self.hidden2(hidden1)</span><br><span class="line">        concat = keras.layers.concatenate([input_A,hidden2])</span><br><span class="line">        aux = self.aux(hidden2)</span><br><span class="line">        main_output = self.main_output(concat)</span><br><span class="line">        <span class="keyword">return</span> main_output, aux</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = myModel()</span><br></pre></td></tr></table></figure>
<h1 id="使用回调函数"><a href="#使用回调函数" class="headerlink" title="使用回调函数"></a>使用回调函数</h1><p><code>fit()</code>方法接收一个callbacks参数，该参数可以指定Keras在训练开始和结束时（或是处理每个批量之前和之后）将调用对象列表。</p>
<p>下面介绍：ModelCheckpoint回调，定期保存模型的检查点；EarlyStopping回调，多个轮次在验证集上没有进展，模型训练就会中断并回滚到最佳模型。</p>
<p>我们也可以通过继承<code>keras.callbacks.Callback</code>，重写其中的方法来自定义自己的回调函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">checkpoint_cb = keras.callbacks.ModelCheckpoint(<span class="string">&#x27;my_model.h5&#x27;</span>,save_best_only=<span class="literal">True</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mse&#x27;</span>,optimizer=keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>))</span><br><span class="line">history = model.fit(X_train,y_train,epochs=<span class="number">20</span>,validation_data=(X_valid,y_valid),callbacks=[checkpoint_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
363/363 [==============================] - 1s 4ms/step - loss: 2.1240 - val_loss: 0.7047
Epoch 2/20
363/363 [==============================] - 1s 4ms/step - loss: 0.6714 - val_loss: 0.6753
Epoch 3/20
363/363 [==============================] - 1s 4ms/step - loss: 0.6299 - val_loss: 0.5984
Epoch 4/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5988 - val_loss: 0.5573
Epoch 5/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5749 - val_loss: 0.5342
Epoch 6/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5565 - val_loss: 0.5197
Epoch 7/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5413 - val_loss: 0.6011
Epoch 8/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5266 - val_loss: 0.4995
Epoch 9/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5164 - val_loss: 0.5106
Epoch 10/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5055 - val_loss: 0.4727
Epoch 11/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4972 - val_loss: 0.5377
Epoch 12/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4884 - val_loss: 0.5494
Epoch 13/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4810 - val_loss: 0.5290
Epoch 14/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4738 - val_loss: 0.4608
Epoch 15/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4683 - val_loss: 0.4647
Epoch 16/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4619 - val_loss: 0.4790
Epoch 17/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4569 - val_loss: 0.4285
Epoch 18/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4517 - val_loss: 0.4691
Epoch 19/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4460 - val_loss: 0.4570
Epoch 20/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4418 - val_loss: 0.4153
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">earlystop = keras.callbacks.EarlyStopping(patience=<span class="number">10</span>,restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mse&#x27;</span>,optimizer=<span class="string">&#x27;sgd&#x27;</span>)</span><br><span class="line">history = model.fit(X_train,y_train,epochs=<span class="number">20</span>,validation_data=(X_valid,y_valid),callbacks=[checkpoint_cb,earlystop])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
363/363 [==============================] - 2s 4ms/step - loss: 0.4373 - val_loss: 0.7789
Epoch 2/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4349 - val_loss: 3.3366
Epoch 3/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5176 - val_loss: 11.5161
Epoch 4/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4472 - val_loss: 5.0992
Epoch 5/20
363/363 [==============================] - 1s 3ms/step - loss: 0.3935 - val_loss: 24.5138
Epoch 6/20
363/363 [==============================] - 1s 4ms/step - loss: 0.7370 - val_loss: 38.8524
Epoch 7/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5221 - val_loss: 368.9537
Epoch 8/20
363/363 [==============================] - 1s 4ms/step - loss: 0.6238 - val_loss: 80.9212
Epoch 9/20
363/363 [==============================] - 1s 4ms/step - loss: 0.5975 - val_loss: 414.6273
Epoch 10/20
363/363 [==============================] - 1s 3ms/step - loss: 0.7681 - val_loss: 190.2743
Epoch 11/20
363/363 [==============================] - 1s 3ms/step - loss: 0.7865 - val_loss: 154.1239
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myCallback</span>(<span class="params">keras.callbacks.Callback</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">on_epoch_end</span>(<span class="params">self,epoch,logs</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\nval/train:&#123;:.2&#125;&#x27;</span>.<span class="built_in">format</span>(logs[<span class="string">&#x27;val_loss&#x27;</span>]/logs[<span class="string">&#x27;loss&#x27;</span>]))</span><br><span class="line">my_cb = myCallback()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mse&#x27;</span>,optimizer=keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>))</span><br><span class="line">history = model.fit(X_train,y_train,epochs=<span class="number">20</span>,validation_data=(X_valid,y_valid),callbacks=[checkpoint_cb,my_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
363/363 [==============================] - 2s 4ms/step - loss: 1.8173 - val_loss: 4.5359

val/train:2.5
Epoch 2/20
363/363 [==============================] - 2s 4ms/step - loss: 0.7135 - val_loss: 1.6776

val/train:2.4
Epoch 3/20
363/363 [==============================] - 1s 3ms/step - loss: 0.6339 - val_loss: 0.6325

val/train:1.0
Epoch 4/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5837 - val_loss: 0.5607

val/train:0.96
Epoch 5/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5529 - val_loss: 0.5251

val/train:0.95
Epoch 6/20
363/363 [==============================] - 1s 3ms/step - loss: 0.5275 - val_loss: 0.5709

val/train:1.1
Epoch 7/20
363/363 [==============================] - 1s 4ms/step - loss: 0.5105 - val_loss: 0.4771

val/train:0.93
Epoch 8/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4933 - val_loss: 0.4701

val/train:0.95
Epoch 9/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4797 - val_loss: 0.4542

val/train:0.95
Epoch 10/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4682 - val_loss: 0.4373

val/train:0.93
Epoch 11/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4588 - val_loss: 0.4422

val/train:0.96
Epoch 12/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4514 - val_loss: 0.4256

val/train:0.94
Epoch 13/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4444 - val_loss: 0.4224

val/train:0.95
Epoch 14/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4384 - val_loss: 0.4104

val/train:0.94
Epoch 15/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4329 - val_loss: 0.4071

val/train:0.94
Epoch 16/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4281 - val_loss: 0.4084

val/train:0.95
Epoch 17/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4234 - val_loss: 0.3962

val/train:0.94
Epoch 18/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4198 - val_loss: 0.3929

val/train:0.94
Epoch 19/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4160 - val_loss: 0.3929

val/train:0.94
Epoch 20/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4127 - val_loss: 0.3911

val/train:0.95
</code></pre><h1 id="微调神经网络超参数"><a href="#微调神经网络超参数" class="headerlink" title="微调神经网络超参数"></a>微调神经网络超参数</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span>(<span class="params">n_hidden=<span class="number">1</span>,n_neurons=<span class="number">30</span>,lr=<span class="number">0.003</span>,input_shape=[<span class="number">8</span>]</span>):</span></span><br><span class="line">    model = keras.models.Sequential()</span><br><span class="line">    model.add(keras.layers.InputLayer(input_shape=input_shape))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_hidden):</span><br><span class="line">        model.add(keras.layers.Dense(n_neurons,activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(keras.layers.Dense(<span class="number">1</span>))</span><br><span class="line">    optimizer = keras.optimizers.SGD(learning_rate=lr)</span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mse&#x27;</span>,optimizer=optimizer)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> reciprocal</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> param = &#123;<span class="string">&#x27;n_hidden&#x27;</span>:[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">          <span class="string">&#x27;n_neurons&#x27;</span>:np.arange(<span class="number">1</span>,<span class="number">100</span>),</span><br><span class="line">          <span class="string">&#x27;lr&#x27;</span>:reciprocal(<span class="number">3e-4</span>,<span class="number">3e-2</span>)&#125;</span><br><span class="line">rnd_search = RandomizedSearchCV(keras_reg,param,n_iter=<span class="number">10</span>,cv=<span class="number">3</span>)</span><br><span class="line">rnd_search.fit(X_train,y_train,epochs=<span class="number">100</span>,validation_data=(X_valid,y_valid),</span><br><span class="line">               callbacks=keras.callbacks.EarlyStopping(patience=<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/100
242/242 [==============================] - 5s 7ms/step - loss: 3.5815 - val_loss: 2.2888
Epoch 2/100
242/242 [==============================] - 1s 6ms/step - loss: 1.7560 - val_loss: 2.5086
Epoch 3/100
242/242 [==============================] - 1s 6ms/step - loss: 1.2191 - val_loss: 1.9225
Epoch 4/100
242/242 [==============================] - 2s 6ms/step - loss: 0.9811 - val_loss: 0.9753
Epoch 5/100
242/242 [==============================] - 2s 8ms/step - loss: 0.8616 - val_loss: 0.8079
Epoch 6/100
242/242 [==============================] - 2s 8ms/step - loss: 0.8030 - val_loss: 0.7575
Epoch 7/100
242/242 [==============================] - 2s 8ms/step - loss: 0.7713 - val_loss: 0.7350
Epoch 8/100
242/242 [==============================] - 2s 7ms/step - loss: 0.7513 - val_loss: 0.7270
Epoch 9/100
242/242 [==============================] - 2s 6ms/step - loss: 0.7336 - val_loss: 0.7480
Epoch 10/100
242/242 [==============================] - 2s 8ms/step - loss: 0.7184 - val_loss: 0.6900

......

Epoch 43/100
363/363 [==============================] - 1s 4ms/step - loss: 0.2850 - val_loss: 0.3304
Epoch 44/100
363/363 [==============================] - 1s 4ms/step - loss: 0.2924 - val_loss: 0.3627
Epoch 45/100
363/363 [==============================] - 1s 4ms/step - loss: 0.2917 - val_loss: 0.2973
Epoch 46/100
363/363 [==============================] - 2s 4ms/step - loss: 0.2852 - val_loss: 0.3063
Epoch 47/100
363/363 [==============================] - 1s 4ms/step - loss: 0.2834 - val_loss: 0.2980





RandomizedSearchCV(cv=3,
                   estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x00000204C9FA0700&gt;,
                   param_distributions=&#123;&#39;lr&#39;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x00000204CA37D970&gt;,
                    &#39;n_hidden&#39;: [0, 1, 2, 3],
                    &#39;n_neurons&#39;: array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,
                                       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,
                                       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,
                                       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,
                                       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,
                                       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])&#125;)
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(rnd_search.best_params_)</span><br><span class="line"><span class="built_in">print</span>(rnd_search.best_score_)</span><br><span class="line">model = rnd_search.best_estimator_.model</span><br></pre></td></tr></table></figure>
<pre><code>&#123;&#39;lr&#39;: 0.006537702460946492, &#39;n_hidden&#39;: 2, &#39;n_neurons&#39;: 97&#125;
-0.3313012421131134
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(model.predict(X_test[:<span class="number">8</span>]))</span><br><span class="line"><span class="built_in">print</span>(y_test[:<span class="number">8</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[[0.4486066]
 [1.2853019]
 [4.6888404]
 [2.4607358]
 [3.0793304]
 [1.7062786]
 [2.475378 ]
 [1.6391015]]
[0.477   0.458   5.00001 2.186   2.78    1.587   1.982   1.575  ]
</code></pre><h1 id="使用Tensorboard可视化"><a href="#使用Tensorboard可视化" class="headerlink" title="使用Tensorboard可视化"></a>使用Tensorboard可视化</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dir</span>():</span></span><br><span class="line">    <span class="keyword">import</span> os</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    root = os.path.join(os.curdir,<span class="string">&#x27;my_logs&#x27;</span>)</span><br><span class="line">    now = time.strftime(<span class="string">&#x27;run_%Y_%m_%d-%H_%M_%S&#x27;</span>)</span><br><span class="line">    log_dir = os.path.join(root,now)</span><br><span class="line">    <span class="keyword">return</span> log_dir</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">run_dir = get_dir()</span><br><span class="line">run_dir</span><br></pre></td></tr></table></figure>
<pre><code>&#39;.\\my_logs\\run_2021_08_19-08_32_56&#39;
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensorboard_cb = keras.callbacks.TensorBoard(run_dir)</span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;mse&#x27;</span>,optimizer=keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>))</span><br><span class="line">model.fit(X_train,y_train,epochs=<span class="number">20</span>,validation_data=(X_valid,y_valid),callbacks=[tensorboard_cb])</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 1/20
363/363 [==============================] - 4s 6ms/step - loss: 1.5561 - val_loss: 1.5187
Epoch 2/20
363/363 [==============================] - 2s 6ms/step - loss: 0.6865 - val_loss: 0.6418
Epoch 3/20
363/363 [==============================] - 2s 6ms/step - loss: 0.6270 - val_loss: 0.6595
Epoch 4/20
363/363 [==============================] - 2s 6ms/step - loss: 0.5937 - val_loss: 0.5843
Epoch 5/20
363/363 [==============================] - 2s 6ms/step - loss: 0.5651 - val_loss: 0.5278
Epoch 6/20
363/363 [==============================] - 2s 6ms/step - loss: 0.5430 - val_loss: 0.5624
Epoch 7/20
363/363 [==============================] - 2s 5ms/step - loss: 0.5248 - val_loss: 0.5022
Epoch 8/20
363/363 [==============================] - 2s 4ms/step - loss: 0.5094 - val_loss: 0.4765
Epoch 9/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4959 - val_loss: 0.4660
Epoch 10/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4840 - val_loss: 0.4719
Epoch 11/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4745 - val_loss: 0.4435
Epoch 12/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4662 - val_loss: 0.4403
Epoch 13/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4587 - val_loss: 0.4307
Epoch 14/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4518 - val_loss: 0.4217
Epoch 15/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4460 - val_loss: 0.4219
Epoch 16/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4410 - val_loss: 0.4171
Epoch 17/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4357 - val_loss: 0.4126
Epoch 18/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4316 - val_loss: 0.4067
Epoch 19/20
363/363 [==============================] - 1s 4ms/step - loss: 0.4276 - val_loss: 0.4016
Epoch 20/20
363/363 [==============================] - 1s 3ms/step - loss: 0.4236 - val_loss: 0.4090
</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> summary</span><br><span class="line">writer = summary.create_file_writer(get_dir())</span><br><span class="line"><span class="keyword">with</span> writer.as_default():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">1000</span>+<span class="number">1</span>):</span><br><span class="line">        summary.scalar(<span class="string">&#x27;my_scalar&#x27;</span>,np.sin(i/<span class="number">10</span>),i)</span><br><span class="line">        data = (np.random.randn(<span class="number">100</span>)+<span class="number">2</span>)*i/<span class="number">1000</span></span><br><span class="line">        summary.histogram(<span class="string">&#x27;my_histogram&#x27;</span>,data,buckets=<span class="number">50</span>,step=i)</span><br><span class="line">        img = np.random.rand(<span class="number">2</span>,<span class="number">32</span>,<span class="number">32</span>,<span class="number">2</span>)</span><br><span class="line">        summary.image(<span class="string">&#x27;my_img&#x27;</span>,img*i/<span class="number">1000</span>,step=i)</span><br><span class="line">        txt = [<span class="string">&#x27;step is &#x27;</span>+<span class="built_in">str</span>(i),<span class="string">&#x27;the square is &#x27;</span>+<span class="built_in">str</span>(i**<span class="number">2</span>)]</span><br><span class="line">        summary.text(<span class="string">&#x27;my_text&#x27;</span>,txt,step=i)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>Keras</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
</search>
